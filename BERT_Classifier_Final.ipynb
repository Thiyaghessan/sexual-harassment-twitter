{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1Fvm-adpXIsa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qhp8BhZ1btcW"
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/final_labels.csv')\n",
    "training_data = data[data['body'].notna()]\n",
    "training_data = data[data['body'].str.len() < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0c6kAAAckdc"
   },
   "outputs": [],
   "source": [
    "# Converting target labels to 1,0\n",
    "X = data['body']\n",
    "label = LabelEncoder()\n",
    "y = label.fit_transform(data['level_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U1KIf2ZAy-c"
   },
   "outputs": [],
   "source": [
    "# Train, validation, test split of training data\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, train_size = 0.8, random_state = 42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_dev, y_dev, train_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR3a6MIbA2e2"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkDEAI1JA4zG"
   },
   "outputs": [],
   "source": [
    "def tensor_inputs(tweets, max_len = 200): \n",
    "    \"\"\"\n",
    "    Converts tweets (str) into tensor inputs for BERT model \n",
    "\n",
    "    Inputs: \n",
    "    tweets: (str) tweets\n",
    "\n",
    "    Outputs: \n",
    "    input_ids: tweets turned into tensors \n",
    "    attention_masks: masks for BERT model \n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    i = 0\n",
    "    for tweet in tweets: \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        tweet, \n",
    "        add_special_tokens = True, \n",
    "        max_length = max_len, \n",
    "        pad_to_max_length = True, \n",
    "        return_attention_mask = True\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a96IQyP9A8cY"
   },
   "outputs": [],
   "source": [
    "# Converting training \n",
    "train_inp, train_mask = inputs(X_train)\n",
    "val_inp, val_mask = inputs(X_valid)\n",
    "train_label = tf.convert_to_tensor(y_train)\n",
    "val_label = tf.convert_to_tensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91HCBY8tBCUj"
   },
   "outputs": [],
   "source": [
    "bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
    "bert.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOWBLoJ-BFDb"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/My Drive/Colab Notebooks/data/CS122'\n",
    "model_save = '/content/drive/My Drive/Colab Notebooks/data/CS122/cs122bert_test.h5'\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save,\n",
    "                                                save_weights_only = True,\n",
    "                                                monitor = 'val_loss',\n",
    "                                                mode = 'min', \n",
    "                                                save_best_only = True), \n",
    "             keras.callbacks.TensorBoard(log_dir = dir)]\n",
    "\n",
    "print('\\nBert Model', bert.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002, epsilon = 1e-08)\n",
    "\n",
    "bert.compile(loss=loss, optimizer=optimizer, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lch0dbdjBNrX"
   },
   "outputs": [],
   "source": [
    "# Fitting BERT model on training data and validating on validation data\n",
    "misogynist = bert.fit([train_inp, train_mask],\n",
    "                      train_label, \n",
    "                      batch_size = 32, \n",
    "                      epochs = 4, \n",
    "                      validation_data = ([val_inp, val_mask], val_label), \n",
    "                      callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81sqyuv4BQm9"
   },
   "outputs": [],
   "source": [
    "# Predicting on test set\n",
    "test_input, _ = tensor_inputs(X_test) \n",
    "test_pred = bert.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlT4UlPPBY8B"
   },
   "outputs": [],
   "source": [
    "# Turning predictions into labels\n",
    "test_prediction = tf.nn.softmax(test_pred.logits)\n",
    "test_prediction = tf.argmax(test_prediction, axis=1).numpy()\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IUhclRgBnaN"
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(test_prediction.T)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJqdo2INBvk5"
   },
   "outputs": [],
   "source": [
    "#Loading target data\n",
    "actual_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/final_data.csv')\n",
    "actua_data_big = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/final_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61MFDR4RBzq1"
   },
   "outputs": [],
   "source": [
    "data_actual = pd.concat([actua_data_big, actual_data])\n",
    "data_actual = data_actual.drop_duplicates(subset = ['candidate_user_name', 'tweet'])\n",
    "data_actual.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PClpk-YlB-nX"
   },
   "outputs": [],
   "source": [
    "predict_input, _ = tensor_inputs(data_actual['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ByGB9JPCNxr"
   },
   "outputs": [],
   "source": [
    "miso_prediction = tf.nn.softmax(test_pred.logits)\n",
    "miso_prediction = tf.argmax(miso_prediction, axis=1).numpy()\n",
    "miso_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fCcgQEnCTWP"
   },
   "outputs": [],
   "source": [
    "data_actual['Predicted'] = tf_prediction.T"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BERT Classifier",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
